\documentclass[a4paper,10pt]{report}
\usepackage[utf8]{inputenc}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage[bookmarksopen, linktocpage]{hyperref}
\setlength{\parindent}{0pt}

\newcommand{\norm}[1]{\lVert#1\rVert} 		%Double vertical bars for norms
\newcommand{\ip}[2]{\langle#1,#2\rangle}	%Inner product
\newcommand{\vb}[1]{\mathbf{#1}}		%Bold vector

\makeatletter
\renewcommand*\env@matrix[1][*\c@MaxMatrixCols c]{%
  \hskip -\arraycolsep
  \let\@ifnextchar\new@ifnextchar
  \array{#1}}
\makeatother
% Augmented matrices. Use like this:
% \begin{pmatrix}[cc|c]

\begin{document}
\title{Differential Equations and Linear Algebra, Spring 2014 Notes}
\author{Zack Garza}
\maketitle
\tableofcontents

\chapter{Vector Spaces}
Next Exam: April 2nd. Covers 4.6$\rightarrow$5.1.

Remember: To show a set spans a space, show that an arbitrary element in that space can be expressed as a linear combination of the vectors in your set. This usually results in a linear system of equations -- as long as you can solve this system in terms of your vectors, it will be consistent.

\section{Bases}
\subsection{Determining a Basis}
A set $S$ that forms a basis for a vector space $V$ must satisfy two conditions:
\begin{enumerate}
  \item $S$ is set of linearly independent vectors.
  \item $S$ spans $V$.
\end{enumerate}

All bases for a given vector space $V$ have the same number of elements, and this is referred to as the
\textit{dimension} of $V$ such that dim[$V$] = the number of elements in the basis.

If the dimension is known, it is equal to the minimum number of vectors needed to span $V$, as well as the maximum number of linearly independent vectors that a set in $V$ can contain.

\textbf{Does a set $S$ form a basis for a vector space $V$?}

First, check for linear independence. If dim[$V$]=$n$ and $S$ contains $n$ linearly independent vectors, $S$ is guaranteed to form a basis for $V$.

This means that given a set of vectors, we only have to shown linear dependence and that the number of elements matches the dimension of the space! So we only need to know their \textbf{linear independence} and the space's \textbf{dimension}. In other words, $n$ linearly independent vectors in $n$-dimensional space means, for free, we have a basis.

Note: dim[$P_n$]$=n+1$.

\textbf{}

\subsubsection{For the subspaces of a given matrix}
\textbf{Null space:}

Suppose we are given a matrix, and want to examine the properties of its null space. Recall that the null space of a matrix $A$ is given by $\left\{ \mathbf{x}: A\mathbf{x} = \mathbf{0}\right\}$, and its dimension is equal to the number of free variables in ref(A).

This is generally found by augmenting the matrix with $\mathbf{0}$ is using ERO to obtain its REF. Once a relationship between the free variables is found, construct a solution set of the form
\begin{align*}
S = \left\{ \mathbf{x}\in \mathbb{R}^n : x = a_1\mathbf{v_1} + a_2\mathbf{v_2} \cdots a_n\mathbf{v_n} \land a_1,a_2,\cdots a_n \in R \right\} = \text{span}\left\{ \text{something}\right\}
\end{align*}

At this point, it should be clear what the basis and dimensions are.

\textbf{Row space:}

A basis for the row space of a matrix $A$ is nothing more than the number of nonzero rows in ref($A$), which is a subspace of $\mathbb{R}^n$.

\textbf{Column space:}

A basis for the column space of a matrix $A$ consists of the column vectors with leading 1s in rref($A$).



\subsection{Extending a Basis}
:o

\section{Inner Product Spaces (4.11)}
\textit{March 24, 2014}\\

  \subsection{Axioms}
    \noindent4 Axioms of an Inner Product
    \begin{enumerate}
      \item
		$\ip{\vb{V_1}}{\vb{V_1}} >= 0$ and $\ip{\vb{V_1}}{\vb{V_1}}=0\iff\vb{V_1} = 0$

		Check that the scalar result is positive or zero, and show that $\ip{\vb{A}}{\vb{A}} = 0$ forces the coefficients to be zero.
      \item
		Commutativity of $\bigodot$

		$\ip{\vb{V_1}}{\vb{V_2}} = \ip{\vb{V_2}}{\vb{V_1}}$
      \item
		Associativity of scalar multiplication over $\bigodot$

		$\ip{c\vb{V_1}}{\vb{V_2}} = c\ip{\vb{V_1}}{\vb{V_2}}$
      \item
		Associativity of $\bigoplus$ over $\bigodot$

		$\ip{\vb{V_1}}{\vb{V_2} + \vb{V_3}} = \ip{\vb{V_1}}{\vb{V_2}} + \ip{\vb{V_1}}{\vb{V_3}}$
    \end{enumerate}

  \subsection{Orthogonality}
    Two vectors $\vb{p}$ and $\vb{q}$ are defined to be orthogonal if $\ip{\vb{p}}{\vb{q}} = 0$.

  \subsection{The Gram-Schmidt Process}
    Given a basis
    \begin{align*}
      S = \left\{\mathbf{v_1, v_2, \cdots v_n}\right\},
    \end{align*}

    the Gram-Schmidt process produces a corresponding orthogonal basis
    \begin{align*}
      S' = \left\{\mathbf{u_1, u_2, \cdots u_n}\right\}
    \end{align*}
    that spans the same vector space as $S$.

    $S'$ is found using the following pattern:
    \begin{align*}
    \mathbf{u_1} &= \mathbf{v_1} \\
    \mathbf{u_2} &= \mathbf{v_2} - \text{proj}_{\mathbf{u_1}} \mathbf{v_2}\\
    \mathbf{u_3} &= \mathbf{v_3} - \text{proj}_{\mathbf{u_1}} \mathbf{v_3} - \text{proj}_{\mathbf{u_2}} \mathbf{v_3}\\
    \end{align*}

    where
    \begin{align*}
      \text{proj}_{\mathbf{u}} \mathbf{v} = (\text{scal}_{\mathbf{u}} \mathbf{v})\frac{\mathbf{u}}{\mathbf{\norm{u}}}
      = \frac{\langle \mathbf{v,u} \rangle}{\norm{\mathbf{u}}}\frac{\mathbf{u}}{\mathbf{\norm{u}}}
      = \frac{\ip{\vb{v}}{\vb{u}}}{\norm{\vb{u}}^2}\vb{u}
    \end{align*}
    is a vector defined as the \textit{orthogonal projection of $\vb{v}$ onto $\vb{u}$.}

      \begin{figure}[htpb]
      \begin{centering}
      \begin{center}
      \includegraphics[width=\linewidth]{./projection.png}
      \label{fig:projection}
      \caption{Orthogonal projection of $\vb{v_2}$ onto $\vb{u_1}$}
      \end{center}
      \par\end{centering}
      \end{figure}

    The orthogonal set $S'$ can then be transformed into an orthonormal set $S''$ by simply dividing the vectors $s\in S'$ by their magnitudes. The usual definition of a vector's magnitude is
    \begin{align*}
    \norm{\vb{a}} = \sqrt{\ip{\vb{a}}{\vb{a}}} \text{ and } \norm{\vb{a}}^2 = \ip{\vb{a}}{\vb{a}}
    \end{align*}

    As a final check, all vectors in $S'$ should be orthogonal to each other, such that
    \begin{align*}
    \ip{\vb{v_i}}{\vb{v_j}} = 0 \text{ when } i \neq j
    \end{align*}

    and all vectors in $S''$ should be orthonormal, such that
    \begin{align*}
    \ip{\vb{v_i}}{\vb{v_j}} = \delta_{ij}
    \end{align*}
\subsection{Types of Questions}
\begin{enumerate}
	\item
		\textit{Is a given set of vectors an orthogonal set?}

		This is only the case if the inner product of every vector with every other vector is zero.

	\item
		\textit{Show a set of vectors is orthonormal (or force it to be)}

		Take the norm of the vector (i.e., the square root of its own inner product). If it is 1, it is a unit vector. Otherwise, divide the vector by its norm to create a an orthonormal set.
\end{enumerate}


\section{Dimension Counting: The Rank-Nullity Theorem}
  Let $A$ be an $m\times n$ matrix, then
  \begin{align*}
    \text{rank}(A) + \text{nullity}(A) = n \text{ (Number of columns)}
  \end{align*}

  Similarly,
  \begin{align*}
   \text{dim[rowspace}(A)] = \text{dim[colspace}(A)] = \text{rank}(A)
  \end{align*}
  However, the row space and column space are subspaces of different vector spaces.



\chapter{Linear Transformations}
%Define mappings

\textbf{Definition: } A mapping $T: V\mapsto W$ is said to be a \textit{linear transformation} if the following properties hold:
\begin{align*}
 T(\vb{u} + \vb{v}) &= T(\vb{u}) + T(\vb{u}) \\
 T(c\vb{v}) &= cT(\vb{v})
\end{align*}

\textbf{Definition: } Let $T: V\mapsto W$ be a linear transformation from one vector space to another, then
\begin{align*}
 \text{kernel}(T)=\left\{\vb{v}\in V | T(\vb{v}) = \vb{0}_w\right\}
\end{align*}


\end{document}